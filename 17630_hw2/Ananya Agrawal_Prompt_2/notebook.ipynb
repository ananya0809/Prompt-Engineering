{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c42a95",
   "metadata": {},
   "source": [
    "# Assignment 1 - In-Context Learning\n",
    "\n",
    "### Ananya Agrawal (ananyaa2)\n",
    "\n",
    "In this assignment, students experiment with in-context learning by selecting and ordering demonstrations to train a large language model at inference time to classify text. In this task, an online store is interested in classifying whether a review describes one or more general topics of interest. The topics are specific to a class of product, in this case vacuum cleaners. Other topics would be relevant to other products.\n",
    "\n",
    "The dataset has been divided into a development, training and test sets. Students should practice setting up their experiments and writing their prompts using only the development set. Demonstrations for in-context leanring can be drawn from the training set. Final evaluation prior to submission should use the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d5a85",
   "metadata": {},
   "source": [
    "# In-Context Learning for Amazon Review Categorization\n",
    "\n",
    "## **Objective**\n",
    "This notebook explores In-Context Learning (ICL) for classifying Amazon reviews into predefined categories.\n",
    "We will experiment with:\n",
    "- Different numbers of demonstrations.\n",
    "- Varying demonstration selections.\n",
    "- Changing demonstration order.\n",
    "\n",
    "## **1. Import Libraries & Load Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515488f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcf8d4",
   "metadata": {},
   "source": [
    "## Load Reviews with Hashtags\n",
    "\n",
    "The dataset is partitioned into development, training and testing sets. While writing the code to setup your experiments and write your prompts, only use the development set. The training set should be used to sample demonstrations. Only when your code is completed and you are ready to turn in your assignment should you run your experiment on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7ac88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Sizes: Dev 100, Train 100, Test 300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Used the product and was very happy with it until about a month ago. Motor sounded like it was working harder; thought maybe I was imagining things. Look all through hoses and brush roller assembly for any blockages. Today it was not getting good suction; then motor suddenly cut back on output. Barely runs; does not run in upright position. No suction. Bought this as an \"inexpensive\" replacement to Dyson that died after 5 years. You get what you pay for evidently. Wondering if manufacturer warranty in effect, though I failed to send in the warranty card.',\n",
       " 'expected': ['#PerformanceAndFunctionality',\n",
       "  '#ValueForMoneyAndInvestment',\n",
       "  '#CustomerExperienceAndExpectations'],\n",
       " 'sentiment': ['N', 'N', 'N']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev = json.load(open('dataset-dev.json', 'r')) # For prompt testing\n",
    "data_train = json.load(open('dataset-train.json', 'r')) # For in-context demonstrations\n",
    "data_test = json.load(open('dataset-test.json', 'r')) # Final evaluation\n",
    "\n",
    "print('\\nDataset Sizes: Dev %i, Train %i, Test %i\\n' % (len(data_dev), len(data_train), len(data_test)))\n",
    "\n",
    "data_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c4bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 training samples, 100 dev samples, and 300 test samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(data_train)} training samples, {len(data_dev)} dev samples, and {len(data_test)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daa5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predefined category list from training data\n",
    "category_set = {tag for example in data_train for tag in example[\"expected\"]}\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accca94f",
   "metadata": {},
   "source": [
    "## **2. Define Prompting Strategy**\n",
    "- Construct few-shot prompts using training examples.\n",
    "- Experiment with different numbers of demonstrations.\n",
    "- Shuffle demonstration order and compare performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b2120",
   "metadata": {},
   "source": [
    "## Define the Hashtag List for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7026c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    '#DesignAndUsabilityIssues',\n",
    "    '#PerformanceAndFunctionality',\n",
    "    '#BatteryAndPowerIssues',\n",
    "    '#DurabilityAndMaterialConcerns',\n",
    "    '#MaintenanceAndCleaning',\n",
    "    '#CustomerExperienceAndExpectations',\n",
    "    '#ValueForMoneyAndInvestment',\n",
    "    '#AssemblyAndSetup'\n",
    "]\n",
    "\n",
    "tag_list = ' '.join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63760d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we validate predictions against this list\n",
    "category_set = set(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d20779",
   "metadata": {},
   "source": [
    "## Review the Hashtag Distribution\n",
    "\n",
    "In general, it is good practice when classifying items to know the distribution of target categories. Categories that are underrepresented, especially in the training data, would lead to underperformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330d23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_demonstrations(k=32, random_order=True):\n",
    "    \"\"\"Sample k examples from training data and format them for in-context learning.\"\"\"\n",
    "    examples = random.sample(data_train, k) if random_order else data_train[:k]\n",
    "\n",
    "    formatted_examples = []\n",
    "    for ex in examples:\n",
    "        formatted_examples.append(f\"Review: {ex['text']}\\nCategories: {', '.join(ex['expected'])}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_examples)\n",
    "\n",
    "def construct_prompt(k=32, random_order=True):\n",
    "    \"\"\"Construct a prompt with sampled demonstrations.\"\"\"\n",
    "    demonstrations = sample_demonstrations(k, random_order)\n",
    "    \n",
    "    prompt_template = (\n",
    "    \"Below are examples of customer reviews categorized into relevant product features.\\n\"\n",
    "    \"Your task is to categorize a new review using only the following hashtags within the same set of categories.:\\n\"\n",
    "    f\"{tag_list}\\n\\n\"\n",
    "    \"Respond with a comma-separated list of valid hashtags from the predefined list.\\n\\n\"\n",
    "    f\"{demonstrations}\\n\"\n",
    "    \"Review: {test_review}\\nCategories:\"\n",
    ")\n",
    "\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb1f3b",
   "metadata": {},
   "source": [
    "## **3. Query Model with Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af7ef3",
   "metadata": {},
   "source": [
    "## Define the Prompt and Experiment\n",
    "\n",
    "The experiment generally has the following steps: (1) sample the training data to identify k demonstrations for 0 =< k < training set size; (2) construct linearize the demonstrations into text; (3) iterate over the test data and insert the test review and text linearization of the demonstrations into the prompt template; (4) send the prompt to the model and receive the response; (5) validate the response, if the response passes then store the response for later, else if the response fails validation, then save the response to a list of errors. It is generally good to save responses and errors with an index that can be linked back to the test data.\n",
    "\n",
    "After running the experiment, the evaluation metrics should be computed from the answers and the errors should be inspected. Adjustments to the prompt and/or experiment can be made to reduce the errors, e.g., by post-processing the responses prior to validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5651415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model(prompt):\n",
    "    \"\"\"Send a structured prompt to OpenAI API and return the response.\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            store=True,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff3d04",
   "metadata": {},
   "source": [
    "## **4. Run Experiments on Dev Dataset & Validate Responses**\n",
    "- Generate model responses using different prompts.\n",
    "- Store predictions for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b72def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response(response):\n",
    "    \"\"\"Filter out invalid responses and hallucinations.\"\"\"\n",
    "    predicted_categories = {tag.strip() for tag in response.split(\",\") if tag.strip().startswith(\"#\")}\n",
    "    return list(predicted_categories.intersection(category_set))\n",
    "\n",
    "def run_experiment(dataset, k=32, random_order=True):\n",
    "    \"\"\"Run prompt-based experiment on a given dataset.\"\"\"\n",
    "    results, errors = [], []\n",
    "\n",
    "    for idx, review in enumerate(dataset):\n",
    "        prompt = construct_prompt(k, random_order).format(test_review=review['text'])\n",
    "        response = prompt_model(prompt)\n",
    "\n",
    "        validated_response = validate_response(response)\n",
    "\n",
    "        if validated_response:\n",
    "            results.append({\n",
    "                \"index\": idx,\n",
    "                \"review\": review[\"text\"],\n",
    "                \"expected\": review[\"expected\"],\n",
    "                \"predicted\": validated_response\n",
    "            })\n",
    "        else:\n",
    "            errors.append({\"index\": idx, \"review\": review[\"text\"], \"response\": response})\n",
    "\n",
    "    return results, errors\n",
    "\n",
    "# Run experiment on dev data for tuning prompt design\n",
    "dev_results, dev_errors = run_experiment(data_dev, k=32, random_order=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6602461",
   "metadata": {},
   "source": [
    "## **5. Evaluate Model Performance on Dev Dataset**\n",
    "- Measure accuracy of category classification and sentiment agreement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858852b9",
   "metadata": {},
   "source": [
    "## Evaluate the Experimental Results\n",
    "\n",
    "The evaluation metrics include precision, recall and F1 score. For the total number of true positives (tp), false positives (fp) and false negatives (fn), these calculations should be used to report results:\n",
    "* Precision = tp / (tp + fp)\n",
    "* Recall = tp / (tp + fn)\n",
    "* F1 = 2tp / (2tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2253158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Evaluation: {'accuracy': 11.0, 'total_reviews': 100}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_results(results):\n",
    "    \"\"\"Calculate accuracy based on exact category matches.\"\"\"\n",
    "    correct = sum(1 for res in results if set(res[\"predicted\"]) == set(res[\"expected\"]))\n",
    "    accuracy = (correct / len(results)) * 100 if results else 0\n",
    "    return {\"accuracy\": accuracy, \"total_reviews\": len(results)}\n",
    "\n",
    "# Evaluate dev results\n",
    "dev_metrics = evaluate_results(dev_results)\n",
    "print(\"Dev Set Evaluation:\", dev_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07681dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, and F1-score for the experiment results.\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0  # Initialize counts\n",
    "    \n",
    "    for res in results:\n",
    "        expected_set = set(res[\"expected\"])\n",
    "        predicted_set = set(res[\"predicted\"])\n",
    "        \n",
    "        tp += len(expected_set & predicted_set)  # True Positives (Correctly predicted)\n",
    "        fp += len(predicted_set - expected_set)  # False Positives (Incorrect predictions)\n",
    "        fn += len(expected_set - predicted_set)  # False Negatives (Missed correct labels)\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": round(precision, 4),\n",
    "        \"recall\": round(recall, 4),\n",
    "        \"f1_score\": round(f1, 4),\n",
    "        \"total_reviews\": len(results)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be9d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Evaluation Metrics: {'precision': 0.6171, 'recall': 0.758, 'f1_score': 0.6803, 'total_reviews': 100}\n"
     ]
    }
   ],
   "source": [
    "dev_metrics = calculate_metrics(dev_results)\n",
    "print(\"Dev Set Evaluation Metrics:\", dev_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acb221",
   "metadata": {},
   "source": [
    "## **6. Run Final Experiment on Test Data & Save Results**\n",
    "- Store experiment outcomes in `results.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33cfd9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Set Evaluation Metrics: {'precision': 0.6169, 'recall': 0.7582, 'f1_score': 0.6803, 'total_reviews': 300}\n"
     ]
    }
   ],
   "source": [
    "# Once we finalize the best prompt strategy from dev, run on test data\n",
    "test_results, test_errors = run_experiment(data_test, k=32, random_order=True)\n",
    "\n",
    "# Evaluate final test results\n",
    "test_metrics = calculate_metrics(test_results)\n",
    "print(\"Final Test Set Evaluation Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f6f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.json saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save final test results to match dataset-test.json format\n",
    "final_results = []\n",
    "for idx, review in enumerate(data_test):\n",
    "    predicted_tags = [res[\"predicted\"] for res in test_results if res[\"index\"] == idx]\n",
    "    \n",
    "    # If no prediction found, default to an empty list\n",
    "    predicted_tags = predicted_tags[0] if predicted_tags else []\n",
    "\n",
    "    # Create a structured dictionary\n",
    "    final_results.append({\n",
    "        \"index\": idx,\n",
    "        \"text\": review[\"text\"],\n",
    "        \"expected\": review[\"expected\"],  # Keep original expected categories\n",
    "        \"predicted\": predicted_tags      # Add model's predicted categories\n",
    "    })\n",
    "\n",
    "# Save results to JSON file\n",
    "with open(\"results.json\", \"a\") as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "print(\"results.json saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25a5d4",
   "metadata": {},
   "source": [
    "## Open Source Models (Optional)\n",
    "\n",
    "If students wish to evaluate their solution on open source models, they may use Ollama, if their hardware supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9909af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import chat\n",
    "# from ollama import ChatResponse\n",
    "\n",
    "# def prompt_ollama(prompt):\n",
    "#     response: ChatResponse = chat(model='llama3.3', messages=[{\n",
    "#         'role': 'user',\n",
    "#         'content': prompt,\n",
    "#       },\n",
    "#     ])\n",
    "#     return response['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
